{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Урок 2. Загрузка данных и построение обучающей выборки. Анализ и предобработка датасета. Балансировка классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реальная задача машинного обучения - это история, прежде всего, про грамотно выстроенные процессы, которые покрывают полный цикл от сбора исходных данных до измеримого бизнес-эффекта. Последовательные преобразования данных и дальнейшее обучение модели на них объединят в процесс, называемый \"пайплайном\" (pipeline). В рамках курса предлагается собрать пайплайн, который будет заключаться в сборе исходных данных, их подготовке к обучению и обучение модели, то есть на входе пайплайна - сырые данные, на выходе - готовая модель. На практике пайплайн часто создают с использованием фреймворка [Apache Spark](https://ru.wikipedia.org/wiki/Apache_Spark).\n",
    "\n",
    "Первый и, пожалуй, самый важный этап пайплайн - сбор данных, специфичен тем, что на нем происходит взаимодействие, как правило, между сервером, обрабатывающим данные (это может быть сервер Jupyter), и хранилищем данных. Особенность заключается в том, что при построении пайплайна нужно учитывать особенности такого взаимодействия. Хранилище может быть недоступно из-за каких-то сетевых проблем, в процессе загрузки может возникнуть какая-то ошибка, в конце концов можно превысить лимиты запрашиваемого объеда данных. Потенциальных проблем может быть много, и хорошо написанный пайплайн должен учитывать, если и не все, то уж точно большинство, возможных исключений и ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности загрузки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на примере загрузки данных с помощью Apache Impala, каким образом можно контролировать загрузку данных и обрабатывать нестандартные ситуации. Для этого обратимся к такому инструменту Python, как __декоратор__.\n",
    "\n",
    "Напомним, что функции в Python являются объектами, соответственно, их можно возвращать из другой функции или передавать в качестве аргумента. Также следует помнить, что функция в Python может быть определена и внутри другой функции. Декораторы — это, по сути, \"обёртки\", которые дают возможность изменить поведение функции, не изменяя её код.\n",
    "\n",
    "Проиллюстрировать смысл декоратора можно следующим кодом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def time_format(sec):\n",
    "    return str(timedelta(seconds=sec))\n",
    "\n",
    "def my_decorator(function_to_decorate):\n",
    "    # Внутри себя декоратор определяет функцию-\"обёртку\". Она будет обёрнута вокруг декорируемой,\n",
    "    # получая возможность исполнять произвольный код до и после неё.\n",
    "    def the_wrapper_around_the_original_function():\n",
    "        print(\"Я - код, который отработает до вызова функции\")\n",
    "        function_to_decorate() # Вызов самой функции\n",
    "        print(\"А я - код, срабатывающий после\")\n",
    "    # Вернём эту функцию\n",
    "    return the_wrapper_around_the_original_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как можно на практике использовать декоратор. Напишем декоратор, который будет измерять время работы функции, наподобии __%%time__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения: 3.0030934810638428 секунды\n",
      "Функция спала ~3 секунды\n"
     ]
    }
   ],
   "source": [
    "def time_it(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        return_value = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print('Время выполнения: {} секунды'.format(end-start))\n",
    "        return return_value\n",
    "    return wrapper\n",
    "\n",
    "@time_it\n",
    "def dummy_func(seconds_to_wait):\n",
    "    time.sleep(seconds_to_wait)\n",
    "    val = 'Функция спала ~{} секунды'.format(seconds_to_wait)\n",
    "    return val\n",
    "\n",
    "dummy = dummy_func(3)\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, применим теперь декораторы к процессу загрузки данных. Напишем следующий декоратор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_loop(func):\n",
    "    def _func(*args, **kwargs):\n",
    "        _max_iter_cnt = 100\n",
    "        for i in range(_max_iter_cnt):\n",
    "            try:\n",
    "                start_t = time.time()\n",
    "                res = func(*args, **kwargs)\n",
    "                run_time = time_format(time.time() - start_t)\n",
    "                print('Run time \"{}\": {}'.format(func.__name__, run_time))\n",
    "                return res\n",
    "            except Exception as er:\n",
    "                run_time = time_format(time.time() - start_t)\n",
    "                print('Run time \"{}\": {}'.format(func.__name__, run_time))\n",
    "                print('-'*50)\n",
    "                print(er, '''Try № {}'''.format(i + 1))\n",
    "                print('-'*50)\n",
    "        raise Exception('Max error limit exceeded: {}'.format(_max_iter_cnt))\n",
    "    return _func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Декоратор говорит нам о том, что декорируемая функция будет выполняться максимум 100 раз, будет отлеживаться время выполнения, в случае ошибки - выводим текст ошибки и номер попытки перезапуска функции, а в случае достижения лимита по итерациям останавливаем выполнение кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, приведем примеры нескольких функций, которые загружают данные на сервер Jupyter и \"обернуты\" декораторами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@etl_loop\n",
    "def loading_step(**kwargs):\n",
    "    with ETL() as etl:\n",
    "        tbl = kwargs['table']\n",
    "        print('Loading -> {}...'.format(tbl))\n",
    "        etl.create_table(sql=getattr(etl, 'CREATE_{}_SQL'.format(tbl.upper())))\n",
    "        etl.insert_table(sql=getattr(etl, '{}_DATA_SQL'.format(tbl.upper())), \n",
    "                         min_level=kwargs['min_level'],\n",
    "                         max_level=kwargs['max_level'], \n",
    "                         churned_start_date=kwargs['churned_start_date'], \n",
    "                         churned_end_date=kwargs['churned_end_date'],\n",
    "                         data_start_date=kwargs['data_start_date'], \n",
    "                         data_end_date=kwargs['data_end_date'])\n",
    "        etl.insert_table(sql=etl.CHECK_SQL, \n",
    "                         tbl_name='usr_erin.churn_{}'.format(tbl), \n",
    "                         is_res_needed=True)\n",
    "        print('Rows = {}, Users = {}'.format(etl.res[0][0], etl.res[0][1]))\n",
    "\n",
    "@etl_loop\n",
    "def save_files(table, path):\n",
    "    with ETL() as etl:\n",
    "        print('Saving \"{}\" into *.csv...'.format(table))\n",
    "        sql_to_file('select * from usr_erin.churn_{}'.format(table), \n",
    "                    '{}{}.csv'.format(path, 'churn_{}'.format(table)))\n",
    "        \n",
    "@etl_loop\n",
    "def load_data(min_level='1',\n",
    "              max_level='100',\n",
    "              churned_start_date='2019-01-01', \n",
    "              churned_end_date=datetime.strftime(datetime.now()-timedelta(days=30), '%Y-%m-%d'),\n",
    "              data_start_date='2019-01-01',\n",
    "              data_end_date=datetime.strftime(datetime.now()-timedelta(days=30), '%Y-%m-%d'), \n",
    "              save_to_csv=True, \n",
    "              start_with='sample',\n",
    "              raw_data_path='../input/'):\n",
    "    \n",
    "    sources = ['sample', 'profiles', 'payments', 'reports', 'abusers', 'logins', 'pings', 'sessions', 'shop']\n",
    "    start_index = sources.index(start_with)\n",
    "    \n",
    "    for tbl in sources[start_index:]:\n",
    "        loading_step(table=tbl, \n",
    "                     min_level=min_level, \n",
    "                     max_level=max_level, \n",
    "                     churned_start_date=churned_start_date, \n",
    "                     churned_end_date=churned_end_date, \n",
    "                     data_start_date=data_start_date, \n",
    "                     data_end_date=data_end_date)\n",
    "        if save_to_csv:\n",
    "            save_files(table=tbl, \n",
    "                       path=raw_data_path)\n",
    "        \n",
    "    print('Done! All data has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что данные загружаются поэтапно, источник за источником, при этом при загрузке каждого из компонентов соединение с Impala инициализируется заново и после отработки каждого из запросов данные сохраняются в csv файл (подумайте, зачем может быть нужна такая поэтапность?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ознакомившись с тонкостями загрузки данных, приступим к созданию датасета из сырых данных, которые были определены на предыдущем уроке. Напишем код, который будет из сырых данных собирать датасет.\n",
    "\n",
    "Напомним, что у нас в распоряжении есть данные за последние 4 недели жизни игрока в проекте, и на основании каждого из \"динамических\" индикаторов мы будем строить по 4 признака, которые будут соответствовать каждой из 4 недель последнего месяца жизни игрока в проекте. Данные дополнительно не будем пока обрабатывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_raw(churned_start_date='2019-01-01', \n",
    "                      churned_end_date='2019-02-01', \n",
    "                      inter_list=[(1,7),(8,14)],\n",
    "                      raw_data_path='train/',\n",
    "                      dataset_path='dataset/', \n",
    "                      mode='train'):\n",
    "    \n",
    "    start_t = time.time()\n",
    " \n",
    "    sample = pd.read_csv('{}sample.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    profiles = pd.read_csv('{}profiles.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    payments = pd.read_csv('{}payments.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    reports = pd.read_csv('{}reports.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    abusers = pd.read_csv('{}abusers.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    logins = pd.read_csv('{}logins.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    pings = pd.read_csv('{}pings.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    sessions = pd.read_csv('{}sessions.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    shop = pd.read_csv('{}shop.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    \n",
    "    print('Run time (reading csv files): {}'.format(time_format(time.time()-start_t)))    \n",
    "#-----------------------------------------------------------------------------------------------------    \n",
    "    print('NO dealing with outliers, missing values and categorical features...')\n",
    "#-----------------------------------------------------------------------------------------------------        \n",
    "    # На основании дня отвала (last_login_dt) строим признаки, которые описывают активность игрока перед уходом\n",
    "    \n",
    "    print('Creating dataset...')\n",
    "    # Создадим пустой датасет - в зависимости от режима построения датасета - train или test\n",
    "    if mode == 'train':\n",
    "        dataset = sample.copy()[['user_id', 'is_churned', 'level', 'donate_total']]\n",
    "    elif mode == 'test':\n",
    "        dataset = sample.copy()[['user_id', 'level', 'donate_total']]\n",
    "\n",
    "    # Пройдемся по всем источникам, содержащим \"динамичекие\" данные\n",
    "    for df in [payments, reports, abusers, logins, pings, sessions, shop]:\n",
    "\n",
    "        # Получим 'day_num_before_churn' для каждого из значений в источнике для определения недели\n",
    "        data = pd.merge(sample[['user_id', 'login_last_dt']], df, on='user_id')\n",
    "        data['day_num_before_churn'] = 1 + (data['login_last_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) - \n",
    "                                data['log_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))).apply(lambda x: x.days)\n",
    "        df_features = data[['user_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # Для каждого признака создадим признаки для каждого из времененно интервала (в нашем примере 4 интервала по 7 дней)\n",
    "        features = list(set(data.columns) - set(['user_id', 'login_last_dt', 'log_dt', 'day_num_before_churn']))\n",
    "        print('Processing with features:', features)\n",
    "        for feature in features:\n",
    "            for i, inter in enumerate(inter_list):\n",
    "                inter_df = data.loc[data['day_num_before_churn'].between(inter[0], inter[1], inclusive=True)].\\\n",
    "                                groupby('user_id')[feature].mean().reset_index().\\\n",
    "                                rename(index=str, columns={feature: feature+'_{}'.format(i+1)})\n",
    "                df_features = pd.merge(df_features, inter_df, how='left', on='user_id')\n",
    "\n",
    "        # Добавляем построенные признаки в датасет\n",
    "        dataset = pd.merge(dataset, df_features, how='left', on='user_id')\n",
    "        \n",
    "        print('Run time (calculating features): {}'.format(time_format(time.time()-start_t)))\n",
    "\n",
    "    # Добавляем \"статические\" признаки\n",
    "    dataset = pd.merge(dataset, profiles, on='user_id')\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "    dataset.to_csv('{}dataset_raw_{}.csv'.format(dataset_path, mode), sep=';', index=False)\n",
    "    print('Dataset is successfully built and saved to {}, run time \"build_dataset_raw\": {}'.\\\n",
    "          format(dataset_path, time_format(time.time()-start_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим написанную функцию к нашим данным, при этом построим датасет отдельно для трейна и теста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Следует из исходных данных\n",
    "CHURNED_START_DATE = '2019-09-01' \n",
    "CHURNED_END_DATE = '2019-10-01'\n",
    "\n",
    "INTER_1 = (1,7)\n",
    "INTER_2 = (8,14)\n",
    "INTER_3 = (15,21)\n",
    "INTER_4 = (22,28)\n",
    "INTER_LIST = [INTER_1, INTER_2, INTER_3, INTER_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time (reading csv files): 0:01:23.901281\n",
      "NO dealing with outliers, missing values and categorical features...\n",
      "Creating dataset...\n",
      "Processing with features: ['trans_amt', 'pay_amt']\n",
      "Run time (calculating features): 0:01:51.854116\n",
      "Processing with features: ['reports_amt']\n",
      "Run time (calculating features): 0:03:22.588922\n",
      "Processing with features: ['sess_with_abusers_amt']\n",
      "Run time (calculating features): 0:07:53.086243\n",
      "Processing with features: ['disconnect_amt', 'session_amt']\n",
      "Run time (calculating features): 0:13:20.591053\n",
      "Processing with features: ['avg_min_ping']\n",
      "Run time (calculating features): 0:18:35.709343\n",
      "Processing with features: ['session_player', 'win_rate', 'leavings_rate', 'kd']\n",
      "Run time (calculating features): 0:23:59.450460\n",
      "Processing with features: ['silver_spent', 'gold_spent']\n",
      "Run time (calculating features): 0:29:20.988729\n",
      "Dataset is successfully built and saved to dataset/, run time \"build_dataset_raw\": 0:29:48.019019\n"
     ]
    }
   ],
   "source": [
    "build_dataset_raw(churned_start_date=CHURNED_START_DATE,\n",
    "                  churned_end_date=CHURNED_END_DATE,\n",
    "                  inter_list=INTER_LIST,\n",
    "                  raw_data_path='train/',\n",
    "                  dataset_path='dataset/', \n",
    "                  mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time (reading csv files): 0:00:05.964428\n",
      "NO dealing with outliers, missing values and categorical features...\n",
      "Creating dataset...\n",
      "Processing with features: ['trans_amt', 'pay_amt']\n",
      "Run time (calculating features): 0:00:08.178363\n",
      "Processing with features: ['reports_amt']\n",
      "Run time (calculating features): 0:00:15.600667\n",
      "Processing with features: ['sess_with_abusers_amt']\n",
      "Run time (calculating features): 0:00:40.310513\n",
      "Processing with features: ['disconnect_amt', 'session_amt']\n",
      "Run time (calculating features): 0:01:08.838487\n",
      "Processing with features: ['avg_min_ping']\n",
      "Run time (calculating features): 0:01:36.332214\n",
      "Processing with features: ['session_player', 'win_rate', 'leavings_rate', 'kd']\n",
      "Run time (calculating features): 0:02:03.799303\n",
      "Processing with features: ['silver_spent', 'gold_spent']\n",
      "Run time (calculating features): 0:02:33.566338\n",
      "Dataset is successfully built and saved to dataset/, run time \"build_dataset_raw\": 0:02:35.930178\n"
     ]
    }
   ],
   "source": [
    "build_dataset_raw(churned_start_date=CHURNED_START_DATE,\n",
    "                  churned_end_date=CHURNED_END_DATE,\n",
    "                  inter_list=INTER_LIST,\n",
    "                  raw_data_path='test/',\n",
    "                  dataset_path='dataset/', \n",
    "                  mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469475, 62) (44764, 61)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('dataset/dataset_raw_train.csv', sep=';')\n",
    "test = pd.read_csv('dataset/dataset_raw_test.csv', sep=';')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_churned</th>\n",
       "      <th>level</th>\n",
       "      <th>donate_total</th>\n",
       "      <th>trans_amt_1</th>\n",
       "      <th>trans_amt_2</th>\n",
       "      <th>trans_amt_3</th>\n",
       "      <th>trans_amt_4</th>\n",
       "      <th>pay_amt_1</th>\n",
       "      <th>pay_amt_2</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_spent_1</th>\n",
       "      <th>gold_spent_2</th>\n",
       "      <th>gold_spent_3</th>\n",
       "      <th>gold_spent_4</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>days_between_reg_fl</th>\n",
       "      <th>days_between_fl_df</th>\n",
       "      <th>has_return_date</th>\n",
       "      <th>has_phone_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e7edd8347e3aaeedf8c494b11240851e3fa0ad231b8f8...</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>88730.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f43cac5f14e06ca039b173e14c323ac0c1fd8492f0cf08...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>44149.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cc7450e0b182947998534ef137b05e07109c100aced0b6...</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>44931.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>21.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c583d57a1e9e53341fc239d41fb6983e667a04b1b4d94...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>37538.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9bbaa1a2501e8dc83cf6c0c54ef139c75c99de09dcf4dc...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4100.97998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.580002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             user_id  is_churned  level  \\\n",
       "0  1e7edd8347e3aaeedf8c494b11240851e3fa0ad231b8f8...           0     43   \n",
       "1  f43cac5f14e06ca039b173e14c323ac0c1fd8492f0cf08...           0     50   \n",
       "2  cc7450e0b182947998534ef137b05e07109c100aced0b6...           0     37   \n",
       "3  5c583d57a1e9e53341fc239d41fb6983e667a04b1b4d94...           0     20   \n",
       "4  9bbaa1a2501e8dc83cf6c0c54ef139c75c99de09dcf4dc...           0     10   \n",
       "\n",
       "   donate_total  trans_amt_1  trans_amt_2  trans_amt_3  trans_amt_4  \\\n",
       "0   88730.00000          NaN          NaN          NaN          NaN   \n",
       "1   44149.00000          NaN          NaN          NaN          NaN   \n",
       "2   44931.00000          1.0          1.0          NaN          2.0   \n",
       "3   37538.00000          NaN          NaN          NaN          NaN   \n",
       "4    4100.97998          1.0          NaN          NaN          NaN   \n",
       "\n",
       "   pay_amt_1  pay_amt_2        ...         gold_spent_1  gold_spent_2  \\\n",
       "0        NaN        NaN        ...             0.000000           0.0   \n",
       "1        NaN        NaN        ...             0.000000           0.0   \n",
       "2  63.000000      350.0        ...           104.285714           0.0   \n",
       "3        NaN        NaN        ...                  NaN           NaN   \n",
       "4  66.580002        NaN        ...             0.000000           0.0   \n",
       "\n",
       "   gold_spent_3  gold_spent_4   age  gender  days_between_reg_fl  \\\n",
       "0     78.666667      0.000000  26.0       M                    0   \n",
       "1      0.000000      0.000000  27.0       M                    0   \n",
       "2      1.428571      2.857143  21.0       M                    0   \n",
       "3           NaN           NaN  22.0       M                    0   \n",
       "4      0.000000      0.000000   2.0       M                    0   \n",
       "\n",
       "   days_between_fl_df  has_return_date  has_phone_number  \n",
       "0                   7                1                 1  \n",
       "1                  37                1                 1  \n",
       "2                 153                1                 1  \n",
       "3                 156                1                 1  \n",
       "4                  21                1                 1  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469475 entries, 0 to 469474\n",
      "Data columns (total 62 columns):\n",
      "user_id                    469475 non-null object\n",
      "is_churned                 469475 non-null int64\n",
      "level                      469475 non-null int64\n",
      "donate_total               469475 non-null float64\n",
      "trans_amt_1                67485 non-null float64\n",
      "trans_amt_2                60928 non-null float64\n",
      "trans_amt_3                56720 non-null float64\n",
      "trans_amt_4                57896 non-null float64\n",
      "pay_amt_1                  67485 non-null float64\n",
      "pay_amt_2                  60928 non-null float64\n",
      "pay_amt_3                  56720 non-null float64\n",
      "pay_amt_4                  57896 non-null float64\n",
      "reports_amt_1              144916 non-null float64\n",
      "reports_amt_2              145909 non-null float64\n",
      "reports_amt_3              147492 non-null float64\n",
      "reports_amt_4              147503 non-null float64\n",
      "sess_with_abusers_amt_1    261313 non-null float64\n",
      "sess_with_abusers_amt_2    259432 non-null float64\n",
      "sess_with_abusers_amt_3    261064 non-null float64\n",
      "sess_with_abusers_amt_4    263303 non-null float64\n",
      "disconnect_amt_1           293492 non-null float64\n",
      "disconnect_amt_2           286263 non-null float64\n",
      "disconnect_amt_3           285912 non-null float64\n",
      "disconnect_amt_4           286945 non-null float64\n",
      "session_amt_1              293492 non-null float64\n",
      "session_amt_2              286263 non-null float64\n",
      "session_amt_3              285912 non-null float64\n",
      "session_amt_4              286945 non-null float64\n",
      "avg_min_ping_1             293275 non-null float64\n",
      "avg_min_ping_2             286095 non-null float64\n",
      "avg_min_ping_3             285812 non-null float64\n",
      "avg_min_ping_4             287043 non-null float64\n",
      "session_player_1           280518 non-null float64\n",
      "session_player_2           275710 non-null float64\n",
      "session_player_3           276624 non-null float64\n",
      "session_player_4           278314 non-null float64\n",
      "win_rate_1                 280518 non-null float64\n",
      "win_rate_2                 275710 non-null float64\n",
      "win_rate_3                 276624 non-null float64\n",
      "win_rate_4                 278314 non-null float64\n",
      "leavings_rate_1            280518 non-null float64\n",
      "leavings_rate_2            275710 non-null float64\n",
      "leavings_rate_3            276624 non-null float64\n",
      "leavings_rate_4            278314 non-null float64\n",
      "kd_1                       280518 non-null float64\n",
      "kd_2                       275710 non-null float64\n",
      "kd_3                       276624 non-null float64\n",
      "kd_4                       278314 non-null float64\n",
      "silver_spent_1             300355 non-null float64\n",
      "silver_spent_2             293676 non-null float64\n",
      "silver_spent_3             292993 non-null float64\n",
      "silver_spent_4             291965 non-null float64\n",
      "gold_spent_1               300355 non-null float64\n",
      "gold_spent_2               293676 non-null float64\n",
      "gold_spent_3               292993 non-null float64\n",
      "gold_spent_4               291965 non-null float64\n",
      "age                        390924 non-null float64\n",
      "gender                     407337 non-null object\n",
      "days_between_reg_fl        469475 non-null int64\n",
      "days_between_fl_df         469475 non-null int64\n",
      "has_return_date            469475 non-null int64\n",
      "has_phone_number           469475 non-null int64\n",
      "dtypes: float64(54), int64(6), object(2)\n",
      "memory usage: 222.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый игрок теперь описывается множеством из N признаков, но в датасете есть пропуски, часть из которых обусловлены пробелами в исходных сырых данных, а часть отсутствует из-за неактивности игрока в тот или иной период времени, поэтому теперь обработаем датасет, чтобы убрать пропуски. Также обработаем явные выбросы и категориальные признаки. Напишем соответствующую функцию, которая делает это.\n",
    "\n",
    "__Важный момент:__ вам предлагается попробовать обработать данные самостоятельно, итоговый препроцессинг зависит только от вас, здесь указан лишь пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, \n",
    "                    dataset_type='train',\n",
    "                    dataset_path='dataset/'):\n",
    "    print(dataset_type)\n",
    "    start_t = time.time()\n",
    "    print('Dealing with missing values, outliers, categorical features...')\n",
    "    \n",
    "    # Профили\n",
    "    dataset['age'] = dataset['age'].fillna(dataset['age'].median())\n",
    "    dataset['gender'] = dataset['gender'].fillna(dataset['gender'].mode()[0])\n",
    "    dataset.loc[~dataset['gender'].isin(['M', 'F']), 'gender'] = dataset['gender'].mode()[0]\n",
    "    dataset['gender'] = dataset['gender'].map({'M': 1., 'F':0.})\n",
    "    dataset.loc[(dataset['age'] > 80) | (dataset['age'] < 7), 'age'] = round(dataset['age'].median())\n",
    "    dataset.loc[dataset['days_between_fl_df'] < -1, 'days_between_fl_df'] = -1\n",
    "    # Пинги\n",
    "    for period in range(1,len(INTER_LIST)+1):\n",
    "        col = 'avg_min_ping_{}'.format(period)\n",
    "        dataset.loc[(dataset[col] < 0) | \n",
    "                    (dataset[col].isnull()), col] = dataset.loc[dataset[col] >= 0][col].median()\n",
    "    # Сессии и прочее\n",
    "    dataset.fillna(0, inplace=True)\n",
    "    dataset.to_csv('{}dataset_{}.csv'.format(dataset_path, dataset_type), sep=';', index=False)\n",
    "         \n",
    "    print('Dataset is successfully prepared and saved to {}, run time (dealing with bad values): {}'.\\\n",
    "          format(dataset_path, time_format(time.time()-start_t))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Dealing with missing values, outliers, categorical features...\n",
      "Dataset is successfully prepared and saved to dataset/, run time (dealing with bad values): 0:00:27.865651\n",
      "test\n",
      "Dealing with missing values, outliers, categorical features...\n",
      "Dataset is successfully prepared and saved to dataset/, run time (dealing with bad values): 0:00:02.552020\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset(dataset=train, dataset_type='train')\n",
    "prepare_dataset(dataset=test, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.read_csv('dataset/dataset_train.csv', sep=';')\n",
    "# test_new = pd.read_csv('dataset/dataset_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469475 entries, 0 to 469474\n",
      "Data columns (total 62 columns):\n",
      "user_id                    469475 non-null object\n",
      "is_churned                 469475 non-null int64\n",
      "level                      469475 non-null int64\n",
      "donate_total               469475 non-null float64\n",
      "trans_amt_1                469475 non-null float64\n",
      "trans_amt_2                469475 non-null float64\n",
      "trans_amt_3                469475 non-null float64\n",
      "trans_amt_4                469475 non-null float64\n",
      "pay_amt_1                  469475 non-null float64\n",
      "pay_amt_2                  469475 non-null float64\n",
      "pay_amt_3                  469475 non-null float64\n",
      "pay_amt_4                  469475 non-null float64\n",
      "reports_amt_1              469475 non-null float64\n",
      "reports_amt_2              469475 non-null float64\n",
      "reports_amt_3              469475 non-null float64\n",
      "reports_amt_4              469475 non-null float64\n",
      "sess_with_abusers_amt_1    469475 non-null float64\n",
      "sess_with_abusers_amt_2    469475 non-null float64\n",
      "sess_with_abusers_amt_3    469475 non-null float64\n",
      "sess_with_abusers_amt_4    469475 non-null float64\n",
      "disconnect_amt_1           469475 non-null float64\n",
      "disconnect_amt_2           469475 non-null float64\n",
      "disconnect_amt_3           469475 non-null float64\n",
      "disconnect_amt_4           469475 non-null float64\n",
      "session_amt_1              469475 non-null float64\n",
      "session_amt_2              469475 non-null float64\n",
      "session_amt_3              469475 non-null float64\n",
      "session_amt_4              469475 non-null float64\n",
      "avg_min_ping_1             469475 non-null float64\n",
      "avg_min_ping_2             469475 non-null float64\n",
      "avg_min_ping_3             469475 non-null float64\n",
      "avg_min_ping_4             469475 non-null float64\n",
      "session_player_1           469475 non-null float64\n",
      "session_player_2           469475 non-null float64\n",
      "session_player_3           469475 non-null float64\n",
      "session_player_4           469475 non-null float64\n",
      "win_rate_1                 469475 non-null float64\n",
      "win_rate_2                 469475 non-null float64\n",
      "win_rate_3                 469475 non-null float64\n",
      "win_rate_4                 469475 non-null float64\n",
      "leavings_rate_1            469475 non-null float64\n",
      "leavings_rate_2            469475 non-null float64\n",
      "leavings_rate_3            469475 non-null float64\n",
      "leavings_rate_4            469475 non-null float64\n",
      "kd_1                       469475 non-null float64\n",
      "kd_2                       469475 non-null float64\n",
      "kd_3                       469475 non-null float64\n",
      "kd_4                       469475 non-null float64\n",
      "silver_spent_1             469475 non-null float64\n",
      "silver_spent_2             469475 non-null float64\n",
      "silver_spent_3             469475 non-null float64\n",
      "silver_spent_4             469475 non-null float64\n",
      "gold_spent_1               469475 non-null float64\n",
      "gold_spent_2               469475 non-null float64\n",
      "gold_spent_3               469475 non-null float64\n",
      "gold_spent_4               469475 non-null float64\n",
      "age                        469475 non-null float64\n",
      "gender                     469475 non-null float64\n",
      "days_between_reg_fl        469475 non-null int64\n",
      "days_between_fl_df         469475 non-null int64\n",
      "has_return_date            469475 non-null int64\n",
      "has_phone_number           469475 non-null int64\n",
      "dtypes: float64(55), int64(6), object(1)\n",
      "memory usage: 222.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_churned</th>\n",
       "      <th>level</th>\n",
       "      <th>donate_total</th>\n",
       "      <th>trans_amt_1</th>\n",
       "      <th>trans_amt_2</th>\n",
       "      <th>trans_amt_3</th>\n",
       "      <th>trans_amt_4</th>\n",
       "      <th>pay_amt_1</th>\n",
       "      <th>pay_amt_2</th>\n",
       "      <th>pay_amt_3</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_spent_1</th>\n",
       "      <th>gold_spent_2</th>\n",
       "      <th>gold_spent_3</th>\n",
       "      <th>gold_spent_4</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>days_between_reg_fl</th>\n",
       "      <th>days_between_fl_df</th>\n",
       "      <th>has_return_date</th>\n",
       "      <th>has_phone_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>4.694750e+05</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "      <td>469475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029448</td>\n",
       "      <td>26.792698</td>\n",
       "      <td>4.800206e+04</td>\n",
       "      <td>0.198513</td>\n",
       "      <td>0.182546</td>\n",
       "      <td>0.173069</td>\n",
       "      <td>0.173995</td>\n",
       "      <td>140.415895</td>\n",
       "      <td>134.262708</td>\n",
       "      <td>129.077459</td>\n",
       "      <td>...</td>\n",
       "      <td>73.334719</td>\n",
       "      <td>72.931138</td>\n",
       "      <td>70.150866</td>\n",
       "      <td>69.289718</td>\n",
       "      <td>26.002226</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>14.340597</td>\n",
       "      <td>218.709164</td>\n",
       "      <td>0.882946</td>\n",
       "      <td>0.830589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.169058</td>\n",
       "      <td>12.680296</td>\n",
       "      <td>8.576742e+04</td>\n",
       "      <td>0.573882</td>\n",
       "      <td>0.567848</td>\n",
       "      <td>0.562879</td>\n",
       "      <td>0.553329</td>\n",
       "      <td>597.573949</td>\n",
       "      <td>603.785240</td>\n",
       "      <td>613.631584</td>\n",
       "      <td>...</td>\n",
       "      <td>380.674537</td>\n",
       "      <td>385.894325</td>\n",
       "      <td>407.242002</td>\n",
       "      <td>385.043766</td>\n",
       "      <td>8.341266</td>\n",
       "      <td>0.248460</td>\n",
       "      <td>114.818230</td>\n",
       "      <td>363.410345</td>\n",
       "      <td>0.321485</td>\n",
       "      <td>0.375115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>6.312000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>2.019600e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>5.543250e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>4.356043e+06</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>44.285714</td>\n",
       "      <td>26.857143</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>56700.000000</td>\n",
       "      <td>56598.857143</td>\n",
       "      <td>46400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>36115.000000</td>\n",
       "      <td>47376.857143</td>\n",
       "      <td>62268.000000</td>\n",
       "      <td>56068.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2534.000000</td>\n",
       "      <td>2683.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          is_churned          level  donate_total    trans_amt_1  \\\n",
       "count  469475.000000  469475.000000  4.694750e+05  469475.000000   \n",
       "mean        0.029448      26.792698  4.800206e+04       0.198513   \n",
       "std         0.169058      12.680296  8.576742e+04       0.573882   \n",
       "min         0.000000      10.000000  6.000000e-02       0.000000   \n",
       "25%         0.000000      17.000000  6.312000e+03       0.000000   \n",
       "50%         0.000000      23.000000  2.019600e+04       0.000000   \n",
       "75%         0.000000      36.000000  5.543250e+04       0.000000   \n",
       "max         1.000000      50.000000  4.356043e+06      18.000000   \n",
       "\n",
       "         trans_amt_2    trans_amt_3    trans_amt_4      pay_amt_1  \\\n",
       "count  469475.000000  469475.000000  469475.000000  469475.000000   \n",
       "mean        0.182546       0.173069       0.173995     140.415895   \n",
       "std         0.567848       0.562879       0.553329     597.573949   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max        44.285714      26.857143      18.000000   56700.000000   \n",
       "\n",
       "           pay_amt_2      pay_amt_3        ...          gold_spent_1  \\\n",
       "count  469475.000000  469475.000000        ...         469475.000000   \n",
       "mean      134.262708     129.077459        ...             73.334719   \n",
       "std       603.785240     613.631584        ...            380.674537   \n",
       "min         0.000000       0.000000        ...              0.000000   \n",
       "25%         0.000000       0.000000        ...              0.000000   \n",
       "50%         0.000000       0.000000        ...              0.000000   \n",
       "75%         0.000000       0.000000        ...              0.000000   \n",
       "max     56598.857143   46400.000000        ...          36115.000000   \n",
       "\n",
       "        gold_spent_2   gold_spent_3   gold_spent_4            age  \\\n",
       "count  469475.000000  469475.000000  469475.000000  469475.000000   \n",
       "mean       72.931138      70.150866      69.289718      26.002226   \n",
       "std       385.894325     407.242002     385.043766       8.341266   \n",
       "min         0.000000       0.000000       0.000000       7.000000   \n",
       "25%         0.000000       0.000000       0.000000      20.000000   \n",
       "50%         0.000000       0.000000       0.000000      24.000000   \n",
       "75%         0.000000       0.000000       0.000000      31.000000   \n",
       "max     47376.857143   62268.000000   56068.000000      80.000000   \n",
       "\n",
       "              gender  days_between_reg_fl  days_between_fl_df  \\\n",
       "count  469475.000000        469475.000000       469475.000000   \n",
       "mean        0.933899            14.340597          218.709164   \n",
       "std         0.248460           114.818230          363.410345   \n",
       "min         0.000000            -1.000000           -1.000000   \n",
       "25%         1.000000             0.000000            8.000000   \n",
       "50%         1.000000             0.000000           56.000000   \n",
       "75%         1.000000             0.000000          266.000000   \n",
       "max         1.000000          2534.000000         2683.000000   \n",
       "\n",
       "       has_return_date  has_phone_number  \n",
       "count    469475.000000     469475.000000  \n",
       "mean          0.882946          0.830589  \n",
       "std           0.321485          0.375115  \n",
       "min           0.000000          0.000000  \n",
       "25%           1.000000          1.000000  \n",
       "50%           1.000000          1.000000  \n",
       "75%           1.000000          1.000000  \n",
       "max           1.000000          1.000000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    455650\n",
       "1     13825\n",
       "Name: is_churned, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new['is_churned'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансировка классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет успешно обработан, пропуски заполнены, явные выбросы обработаны. Однако, в обучающей выборке видим, что присутствует сильный дизбаланс классов - __3/97__. Сбалансируем классы с помощью библиотеки __imblearn__.\n",
    "\n",
    "В данной библиотеки, как и в целом в балансировке блассов, есть две основные категории методов: __Under-sampling__ и __Over-sampling__. Для \"сглаживания\" дизбаланса между классами самой интуитивной мерой было бы сделать овер-сэмплинг минорного класса, это можно было бы сделать методом, например, ADASYN (см. [документацию](https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.over_sampling)), но что если одновременно с нехваткой объектов минорного класса в датасете присутствует некоторое количество \"излишних\" объектов мажорного класса? То есть имеются объекты, у которых вектора признаков очень схожи между собой. Значит, можно убрать часть таких объектов и не потерять в \"информативности\" выборки. Для решения этой задачи есть ряд методов, например, ClusterCentroids, при котором кластеры схожих объектов в многомерном пространстве объектов признаков заменяются одним синтетическим объектом - центроидом кластера (см. [документацию](https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.under_sampling)).\n",
    "\n",
    "Для того, чтобы одновременно убрать потенциальные \"излишки\" в мажорном классе и одновременно увеличить размер минорного класса, можно использовать комбинации under- и over-sampling методов. Для этого есть соответствующие методы __combine.SMOTEENN__ и __combine.SMOTETomek__ (см. [документацию](https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine)).\n",
    "\n",
    "Применим SMOTE для нашей задачи и посмотрим, как изменится баланс классов. SMOTE использует Евклидово расстояние, как метрику близости объектов, для создания синтетических объектов минорного класса, поэтому перед его применением следует отмасштабировать признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_new.drop(['user_id', 'is_churned'], axis=1)\n",
    "y_train = train_new['is_churned']\n",
    "\n",
    "X_train_mm = MinMaxScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.4 s, sys: 2.03 s, total: 37.4 s\n",
      "Wall time: 9.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_balanced, y_train_balanced = SMOTE(random_state=42, ratio=0.3). \\\n",
    "                                        fit_sample(X_train_mm, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним баланс до и после:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До: Counter({0: 455650, 1: 13825})\n",
      "После: Counter({0: 455650, 1: 136695})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('До:', Counter(y_train.values))\n",
    "print('После:', Counter(y_train_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидали, баланс выровнялся до соотношения 23/77 (=0.3), как мы и указали в параметрах SMOTE (ratio=0.3).\n",
    "\n",
    "Балансировка классов позволяет осуществлять __trade-off__ между precision и recall: если нужно максимизировать полноту, а целевой класс минорный, то делаем over-sampling до соотношения классов 1:1 или даже больше, чтобы модель \"видела\" много объектов целевого класса и хорошо обучилась на них, но в ущерб precision, так как вместе с recall будет расти количество ошибок I рода.\n",
    "\n",
    "Обычно для максимизации F1 достаточно лишь немного восполнить баланс классов до соотношения __20%-80%__.\n",
    "\n",
    "При балансировке классов важно помнить, что балансировать нужно только train в обучающей выборке, а test в обучающей выборке оставить без изменений, тогда валидация будет корректной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Понимаем декораторы в Python'e, шаг за шагом. Шаг 1](https://habr.com/ru/post/141411/)\n",
    "2. [Primer on Python Decorators](https://realpython.com/primer-on-python-decorators/)\n",
    "3. [Decorators in Python](https://www.geeksforgeeks.org/decorators-in-python/)\n",
    "4. [Apache Spark™](https://spark.apache.org/)\n",
    "5. [imbalanced-learn API](https://imbalanced-learn.readthedocs.io/en/stable/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Д/З"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Напишите функцию, которая возвращает сумму двух вещественных аргументов a и b, а к ней декоратор, который делает так, чтобы возвращаемое значение функцией было по модулю 5.\n",
    "2. Попробуйте описать своими словами основные отличия SMOTE от ADASYN, ознакомившись с документацией к ним."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
